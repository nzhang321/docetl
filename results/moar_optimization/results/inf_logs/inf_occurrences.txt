
                ================================================================================
                Timestamp: 2026-01-25 18:52:25
                Node ID: 1
                Parent ID: 0
                Latest Action: None
                Failure Type: execution_failure
                YAML Path: C:\Users\nickz\cuhk_app\docetl\results\moar_optimization\results\pipeline_openai_gpt-5-nano.yaml
                Error Message: 'remaining_time'
                ================================================================================
                
                ================================================================================
                Timestamp: 2026-01-25 18:52:25
                Node ID: 1
                Parent ID: 0
                Latest Action: None
                Failure Type: simulation_execution_failure
                YAML Path: C:\Users\nickz\cuhk_app\docetl\results\moar_optimization\results\pipeline_openai_gpt-5-nano.yaml
                Error Message: Failed to execute plan C:\Users\nickz\cuhk_app\docetl\results\moar_optimization\results\pipeline_openai_gpt-5-nano.yaml: 'remaining_time'
                ================================================================================
                
                ================================================================================
                Timestamp: 2026-01-26 08:42:46
                Node ID: 1
                Parent ID: 0
                Latest Action: None
                Failure Type: execution_failure
                YAML Path: C:\Users\nickz\cuhk_app\docetl\results\moar_optimization\results\pipeline_openai_gpt-5-mini.yaml
                Error Message: 'remaining_time'
                ================================================================================
                
                ================================================================================
                Timestamp: 2026-01-26 08:42:46
                Node ID: 1
                Parent ID: 0
                Latest Action: None
                Failure Type: simulation_execution_failure
                YAML Path: C:\Users\nickz\cuhk_app\docetl\results\moar_optimization\results\pipeline_openai_gpt-5-mini.yaml
                Error Message: Failed to execute plan C:\Users\nickz\cuhk_app\docetl\results\moar_optimization\results\pipeline_openai_gpt-5-mini.yaml: 'remaining_time'
                ================================================================================
                
                ================================================================================
                Timestamp: 2026-01-26 09:54:32
                Node ID: 2-1-acc
                Parent ID: 2
                Latest Action: name='doc_chunking' formal_description='Map => Split -> Gather -> [Sample] -> Map -> Reduce' nl_description='Transforms a single Map operation into a chunking pipeline: splits long documents into chunks, gathers context around each chunk, optionally samples a subset of chunks for efficiency, processes chunks with a new Map operation, then reduces the results. By default, sampling is applied unless the task requires processing ALL chunks. This directive can only be applied to a top-level Map operation, not to a sub-map within a pipeline that already contains a split, gather, or reduce sequence.' when_to_use="Use when you need to process long documents to extract information, and the document is too long for a single Map operation. The agent will automatically decide whether to sample chunks (for tasks like categorization, theme extraction) or process all chunks (for comprehensive extraction of all instances). Do not apply if the target operation is already part of a split -> gather -> map -> reduce pipeline. Use different gather configs: 'previous.head' for documents with key metadata/definitions at the start, 'previous.tail' for maintaining references, and 'next.head' only for tables/clauses spanning chunks." instantiate_schema_type=<class 'docetl.reasoning_optimizer.instantiate_schemas.DocumentChunkingInstantiateSchema'> example='\n            Original Op (MapOpConfig):\n            - name: extract_contract_terms\n              type: map\n              prompt: |\n                Extract all payment terms, deadlines, and penalty clauses from the contract:\n                {{ input.contract_text }}\n                Return a comprehensive list of all terms found.\n              output:\n                schema:\n                  contract_terms: list[str]\n\n            Example InstantiateSchema Options (what the agent should output):\n\n            # Basic context - good for most cases:\n            {\n              "chunk_size": 10000,\n              "split_key": "contract_text",\n              "sub_prompt": "You are analyzing a chunk of a larger document. Extract all payment terms, deadlines, and penalty clauses from this contract chunk: {{ input.contract_text_chunk_rendered }}. Return a comprehensive list of all terms found.",\n              "reduce_prompt": "Combine results from multiple document chunks: Extract all payment terms, deadlines, and penalty clauses by combining the results from each chunk: {% for input in inputs %}{{ input.contract_terms | join(\', \') }}{% if not loop.last %}, {% endif %}{% endfor %}. Remove duplicates and return a comprehensive list of all terms found.",\n              "gather_config": {\n                "previous": {\n                  "tail": {\n                    "count": 0.5\n                  }\n                }\n              },\n            }\n\n            # Rich context - for complex documents needing document-level metadata:\n            {\n              "gather_config": {\n                "previous": {\n                  "head": {\n                    "count": 1,\n                    "content_key": "contract_text_chunk"\n                  },\n                  "tail": {\n                    "count": 2,\n                    "content_key": "contract_text_chunk"\n                  }\n                }\n              }\n            }\n\n            # Forward context - for tables or clauses spanning chunks:\n            {\n              "gather_config": {\n                "previous": {\n                  "tail": {\n                    "count": 1,\n                    "content_key": "contract_text_chunk"\n                  }\n                },\n                "next": {\n                  "head": {\n                    "count": 1,\n                    "content_key": "contract_text_chunk"\n                  }\n                }\n              }\n            }\n        ' test_cases=[DirectiveTestCase(name='comprehensive_legal_analysis', description='Should transform complex legal document analysis into chunking pipeline', input_config={'name': 'analyze_legal_document', 'type': 'map', 'prompt': 'From this legal document, extract all liability clauses with risk ratings (1-10), identify all parties and their obligations, find all monetary amounts with currencies, extract all dates and deadlines with legal consequences, and list all governing laws or jurisdictions mentioned. For each liability clause, assess the risk level considering industry standards and provide specific reasoning. Group findings by document section if clearly indicated. Return comprehensive analysis ensuring no critical legal elements are missed: {{ input.legal_document }}', 'output': {'schema': {'liability_analysis': 'list[str]', 'parties_obligations': 'list[str]', 'financial_terms': 'list[str]', 'critical_dates': 'list[str]', 'governing_laws': 'list[str]', 'risk_assessment': 'str'}}}, target_ops=['analyze_legal_document'], expected_behavior='Should create chunking pipeline where sub_prompt covers all extraction tasks (liability, parties, financial, dates, laws) with same risk assessment criteria, and reduce_prompt aggregates all findings maintaining the complete analytical framework and output schema from original prompt', should_pass=True), DirectiveTestCase(name='clinical_trial_comprehensive_extraction', description='Should transform detailed clinical research analysis into chunking pipeline', input_config={'name': 'extract_clinical_data', 'type': 'map', 'prompt': "Analyze this clinical trial document and extract: primary and secondary endpoints with measurement criteria, all adverse events categorized by severity (mild/moderate/severe), patient demographics including inclusion/exclusion criteria, statistical significance results with p-values and confidence intervals, drug dosages and administration protocols, and study methodology details. For each adverse event, determine if it's treatment-related based on temporal relationship and biological plausibility. Calculate overall safety profile score (1-10) considering frequency and severity of events. Ensure all regulatory compliance elements are captured: {{ input.trial_document }}", 'output': {'schema': {'endpoints': 'list[str]', 'adverse_events': 'list[str]', 'demographics': 'str', 'statistical_results': 'list[str]', 'protocols': 'list[str]', 'safety_assessment': 'str', 'compliance_status': 'str'}}}, target_ops=['extract_clinical_data'], expected_behavior='Should create chunking pipeline where sub_prompt preserves all clinical analysis requirements (endpoints, adverse events, demographics, statistics, protocols) with same assessment criteria, and reduce_prompt combines results maintaining complete clinical framework and safety scoring methodology from original prompt', should_pass=True), DirectiveTestCase(name='financial_comprehensive_analysis', description='Should transform complex financial document analysis into chunking pipeline', input_config={'name': 'analyze_financial_report', 'type': 'map', 'prompt': 'From this annual financial report, extract all revenue streams with growth rates and market segments, identify all risk factors with impact assessments (low/medium/high), find all forward-looking statements and their associated uncertainties, extract key financial ratios and calculate trend analysis over mentioned periods, identify all subsidiaries with their contribution to consolidated results, and analyze competitive positioning statements. For each risk factor, assess potential financial impact in dollar ranges and likelihood percentages. Ensure all material information affecting investor decisions is captured and categorized by urgency level: {{ input.financial_report }}', 'output': {'schema': {'revenue_analysis': 'list[str]', 'risk_factors': 'list[str]', 'forward_statements': 'list[str]', 'financial_ratios': 'str', 'subsidiaries': 'list[str]', 'competitive_analysis': 'str', 'material_disclosures': 'list[str]'}}}, target_ops=['analyze_financial_report'], expected_behavior='Should create chunking pipeline where sub_prompt maintains all financial analysis requirements (revenue, risks, statements, ratios, subsidiaries, competitive analysis) with same impact assessment methodology, and reduce_prompt aggregates preserving complete financial analytical framework and materiality assessments from original prompt', should_pass=True)]
                Failure Type: execution_failure
                YAML Path: C:\Users\nickz\cuhk_app\docetl\results\moar_optimization\results\pipeline_openai_2-1-acc.yaml
                Error Message: 'full_content_chunk'
                ================================================================================
                
                ================================================================================
                Timestamp: 2026-01-26 09:54:32
                Node ID: 2-1-acc
                Parent ID: 2
                Latest Action: name='doc_chunking' formal_description='Map => Split -> Gather -> [Sample] -> Map -> Reduce' nl_description='Transforms a single Map operation into a chunking pipeline: splits long documents into chunks, gathers context around each chunk, optionally samples a subset of chunks for efficiency, processes chunks with a new Map operation, then reduces the results. By default, sampling is applied unless the task requires processing ALL chunks. This directive can only be applied to a top-level Map operation, not to a sub-map within a pipeline that already contains a split, gather, or reduce sequence.' when_to_use="Use when you need to process long documents to extract information, and the document is too long for a single Map operation. The agent will automatically decide whether to sample chunks (for tasks like categorization, theme extraction) or process all chunks (for comprehensive extraction of all instances). Do not apply if the target operation is already part of a split -> gather -> map -> reduce pipeline. Use different gather configs: 'previous.head' for documents with key metadata/definitions at the start, 'previous.tail' for maintaining references, and 'next.head' only for tables/clauses spanning chunks." instantiate_schema_type=<class 'docetl.reasoning_optimizer.instantiate_schemas.DocumentChunkingInstantiateSchema'> example='\n            Original Op (MapOpConfig):\n            - name: extract_contract_terms\n              type: map\n              prompt: |\n                Extract all payment terms, deadlines, and penalty clauses from the contract:\n                {{ input.contract_text }}\n                Return a comprehensive list of all terms found.\n              output:\n                schema:\n                  contract_terms: list[str]\n\n            Example InstantiateSchema Options (what the agent should output):\n\n            # Basic context - good for most cases:\n            {\n              "chunk_size": 10000,\n              "split_key": "contract_text",\n              "sub_prompt": "You are analyzing a chunk of a larger document. Extract all payment terms, deadlines, and penalty clauses from this contract chunk: {{ input.contract_text_chunk_rendered }}. Return a comprehensive list of all terms found.",\n              "reduce_prompt": "Combine results from multiple document chunks: Extract all payment terms, deadlines, and penalty clauses by combining the results from each chunk: {% for input in inputs %}{{ input.contract_terms | join(\', \') }}{% if not loop.last %}, {% endif %}{% endfor %}. Remove duplicates and return a comprehensive list of all terms found.",\n              "gather_config": {\n                "previous": {\n                  "tail": {\n                    "count": 0.5\n                  }\n                }\n              },\n            }\n\n            # Rich context - for complex documents needing document-level metadata:\n            {\n              "gather_config": {\n                "previous": {\n                  "head": {\n                    "count": 1,\n                    "content_key": "contract_text_chunk"\n                  },\n                  "tail": {\n                    "count": 2,\n                    "content_key": "contract_text_chunk"\n                  }\n                }\n              }\n            }\n\n            # Forward context - for tables or clauses spanning chunks:\n            {\n              "gather_config": {\n                "previous": {\n                  "tail": {\n                    "count": 1,\n                    "content_key": "contract_text_chunk"\n                  }\n                },\n                "next": {\n                  "head": {\n                    "count": 1,\n                    "content_key": "contract_text_chunk"\n                  }\n                }\n              }\n            }\n        ' test_cases=[DirectiveTestCase(name='comprehensive_legal_analysis', description='Should transform complex legal document analysis into chunking pipeline', input_config={'name': 'analyze_legal_document', 'type': 'map', 'prompt': 'From this legal document, extract all liability clauses with risk ratings (1-10), identify all parties and their obligations, find all monetary amounts with currencies, extract all dates and deadlines with legal consequences, and list all governing laws or jurisdictions mentioned. For each liability clause, assess the risk level considering industry standards and provide specific reasoning. Group findings by document section if clearly indicated. Return comprehensive analysis ensuring no critical legal elements are missed: {{ input.legal_document }}', 'output': {'schema': {'liability_analysis': 'list[str]', 'parties_obligations': 'list[str]', 'financial_terms': 'list[str]', 'critical_dates': 'list[str]', 'governing_laws': 'list[str]', 'risk_assessment': 'str'}}}, target_ops=['analyze_legal_document'], expected_behavior='Should create chunking pipeline where sub_prompt covers all extraction tasks (liability, parties, financial, dates, laws) with same risk assessment criteria, and reduce_prompt aggregates all findings maintaining the complete analytical framework and output schema from original prompt', should_pass=True), DirectiveTestCase(name='clinical_trial_comprehensive_extraction', description='Should transform detailed clinical research analysis into chunking pipeline', input_config={'name': 'extract_clinical_data', 'type': 'map', 'prompt': "Analyze this clinical trial document and extract: primary and secondary endpoints with measurement criteria, all adverse events categorized by severity (mild/moderate/severe), patient demographics including inclusion/exclusion criteria, statistical significance results with p-values and confidence intervals, drug dosages and administration protocols, and study methodology details. For each adverse event, determine if it's treatment-related based on temporal relationship and biological plausibility. Calculate overall safety profile score (1-10) considering frequency and severity of events. Ensure all regulatory compliance elements are captured: {{ input.trial_document }}", 'output': {'schema': {'endpoints': 'list[str]', 'adverse_events': 'list[str]', 'demographics': 'str', 'statistical_results': 'list[str]', 'protocols': 'list[str]', 'safety_assessment': 'str', 'compliance_status': 'str'}}}, target_ops=['extract_clinical_data'], expected_behavior='Should create chunking pipeline where sub_prompt preserves all clinical analysis requirements (endpoints, adverse events, demographics, statistics, protocols) with same assessment criteria, and reduce_prompt combines results maintaining complete clinical framework and safety scoring methodology from original prompt', should_pass=True), DirectiveTestCase(name='financial_comprehensive_analysis', description='Should transform complex financial document analysis into chunking pipeline', input_config={'name': 'analyze_financial_report', 'type': 'map', 'prompt': 'From this annual financial report, extract all revenue streams with growth rates and market segments, identify all risk factors with impact assessments (low/medium/high), find all forward-looking statements and their associated uncertainties, extract key financial ratios and calculate trend analysis over mentioned periods, identify all subsidiaries with their contribution to consolidated results, and analyze competitive positioning statements. For each risk factor, assess potential financial impact in dollar ranges and likelihood percentages. Ensure all material information affecting investor decisions is captured and categorized by urgency level: {{ input.financial_report }}', 'output': {'schema': {'revenue_analysis': 'list[str]', 'risk_factors': 'list[str]', 'forward_statements': 'list[str]', 'financial_ratios': 'str', 'subsidiaries': 'list[str]', 'competitive_analysis': 'str', 'material_disclosures': 'list[str]'}}}, target_ops=['analyze_financial_report'], expected_behavior='Should create chunking pipeline where sub_prompt maintains all financial analysis requirements (revenue, risks, statements, ratios, subsidiaries, competitive analysis) with same impact assessment methodology, and reduce_prompt aggregates preserving complete financial analytical framework and materiality assessments from original prompt', should_pass=True)]
                Failure Type: simulation_execution_failure
                YAML Path: C:\Users\nickz\cuhk_app\docetl\results\moar_optimization\results\pipeline_openai_2-1-acc.yaml
                Error Message: Failed to execute plan C:\Users\nickz\cuhk_app\docetl\results\moar_optimization\results\pipeline_openai_2-1-acc.yaml: 'full_content_chunk'
                ================================================================================
                
                ================================================================================
                Timestamp: 2026-01-26 10:04:48
                Node ID: 2-2-acc
                Parent ID: 2
                Latest Action: name='deterministic_doc_compression' formal_description='Op => Code Map -> Op' nl_description='Reduces LLM processing costs by using deterministic logic (regex, patterns) to compress documents before expensive downstream operations, removing irrelevant content that could distract the LLM' when_to_use='When documents contain identifiable patterns or keywords and you want to reduce token costs for downstream LLM operations while improving accuracy by eliminating distracting irrelevant content' instantiate_schema_type=<class 'docetl.reasoning_optimizer.instantiate_schemas.DeterministicDocCompressionInstantiateSchema'> example='\n        Target Operations:\n        - name: analyze_regulatory_compliance\n          type: map\n          prompt: |\n            Analyze regulatory compliance issues in this legal document: {{ input.legal_document }}\n            Focus on identifying violations, required actions, and compliance deadlines.\n          output:\n            schema:\n              violations: "list[str]"\n              required_actions: "list[str]"\n              deadlines: "list[str]"\n\n        Example InstantiateSchema (what the agent should output):\n        {\n        "name": "extract_compliance_sections",\n        "code": \'\'\'\ndef transform(input_doc):\n    import re\n\n    legal_document = input_doc.get(\'legal_document\', \'\')\n\n    # Patterns to identify compliance-related content\n    compliance_patterns = [\n        r\'(?i)(violat[e|ion]|breach|non-complian[ce|t])\',\n        r\'(?i)(deadline|due date|expir[e|ation]|within.*days?)\',\n        r\'(?i)(shall|must|required|mandatory|obligation)\',\n        r\'(?i)(section|article|clause)\\s+\\d+.*(complian[ce|t]|regulat[e|ory])\'\n    ]\n\n    relevant_spans = []\n\n    # Find all matches and extract context around them\n    for pattern in compliance_patterns:\n        for match in re.finditer(pattern, legal_document):\n            start_pos = match.start()\n            end_pos = match.end()\n\n            # Extract 300 chars before and 800 chars after the match\n            context_start = max(0, start_pos - 300)\n            context_end = min(len(legal_document), end_pos + 800)\n\n            # Extract the context around the match\n            context = legal_document[context_start:context_end]\n            relevant_spans.append((context_start, context_end, context))\n\n    # Merge overlapping spans and remove duplicates\n    if relevant_spans:\n        # Sort by start position\n        relevant_spans.sort(key=lambda x: x[0])\n        merged_spans = [relevant_spans[0]]\n\n        for current_start, current_end, current_text in relevant_spans[1:]:\n            last_start, last_end, last_text = merged_spans[-1]\n\n            if current_start <= last_end + 100:  # Merge if close enough\n                # Extend the last span\n                new_end = max(last_end, current_end)\n                new_text = legal_document[last_start:new_end]\n                merged_spans[-1] = (last_start, new_end, new_text)\n            else:\n                merged_spans.append((current_start, current_end, current_text))\n\n        # Extract just the text portions\n        compressed_text = \'\\n\\n--- SECTION BREAK ---\\n\\n\'.join([span[2] for span in merged_spans])\n    else:\n        compressed_text = legal_document  # Fallback if no matches\n\n    return {\n        \'legal_document\': compressed_text\n    }\n            \'\'\'\n        }\n        ' test_cases=[DirectiveTestCase(name='detailed_merger_agreement_analysis', description='Should compress merger agreement for comprehensive legal analysis', input_config={'name': 'analyze_merger_agreement_terms', 'type': 'map', 'prompt': 'Perform a comprehensive legal analysis of this merger agreement: {{ input.merger_agreement }}\n\n                    Analyze and extract the following:\n                    1. Purchase price structure and payment mechanisms (cash, stock, earnouts, escrow arrangements)\n                    2. Material adverse change (MAC) definitions and carve-outs that could affect deal completion\n                    3. Representations and warranties with survival periods and liability caps\n                    4. Closing conditions precedent, including regulatory approvals and third-party consents\n                    5. Termination rights and associated breakup fees or reverse breakup fees\n                    6. Indemnification provisions including baskets, caps, and survival periods\n                    7. Employee retention arrangements and change-in-control provisions\n                    8. Integration planning requirements and operational restrictions during pendency\n                    9. Dispute resolution mechanisms and governing law provisions\n                    10. Post-closing adjustments and working capital mechanisms\n\n                    For each area, provide specific clause references, dollar amounts where applicable,\n                    time periods, and risk assessment (High/Medium/Low) with justification.', 'output': {'schema': {'purchase_price_analysis': 'string', 'mac_provisions': 'list[str]', 'representations_warranties': 'list[str]', 'closing_conditions': 'list[str]', 'termination_rights': 'string', 'indemnification_terms': 'string', 'employee_provisions': 'list[str]', 'integration_restrictions': 'list[str]', 'dispute_resolution': 'string', 'post_closing_adjustments': 'string', 'risk_assessment': 'string'}}}, target_ops=['analyze_merger_agreement_terms'], expected_behavior="Should add Code Map operation that extracts merger agreement sections using regex patterns for legal terms, financial provisions, and risk-related clauses. The return dictionary of the transform function should be {'merger_agreement': ....} only.", should_pass=True), DirectiveTestCase(name='multi_document_analysis_compression', description='Should compress document for multiple analysis operations', input_config=[{'name': 'extract_financial_metrics', 'type': 'map', 'prompt': 'Extract revenue, profit, and expense figures from: {{ input.earnings_report }}', 'output': {'schema': {'revenue': 'string', 'profit': 'string', 'expenses': 'list[str]'}}}, {'name': 'assess_financial_risks', 'type': 'map', 'prompt': 'Identify financial risks and warning signs in: {{ input.earnings_report }}', 'output': {'schema': {'risks': 'list[str]', 'warning_signs': 'list[str]'}}}], target_ops=['extract_financial_metrics', 'assess_financial_risks'], expected_behavior="Should add Code Map operation that extracts financial content needed for both operations. The return dictionary of the transform function should be {'earnings_report': ....} only.", should_pass=True)]
                Failure Type: execution_failure
                YAML Path: C:\Users\nickz\cuhk_app\docetl\results\moar_optimization\results\pipeline_openai_2-2-acc.yaml
                Error Message: invalid group reference 1 at position 198
                ================================================================================
                
                ================================================================================
                Timestamp: 2026-01-26 10:04:48
                Node ID: 2-2-acc
                Parent ID: 2
                Latest Action: name='deterministic_doc_compression' formal_description='Op => Code Map -> Op' nl_description='Reduces LLM processing costs by using deterministic logic (regex, patterns) to compress documents before expensive downstream operations, removing irrelevant content that could distract the LLM' when_to_use='When documents contain identifiable patterns or keywords and you want to reduce token costs for downstream LLM operations while improving accuracy by eliminating distracting irrelevant content' instantiate_schema_type=<class 'docetl.reasoning_optimizer.instantiate_schemas.DeterministicDocCompressionInstantiateSchema'> example='\n        Target Operations:\n        - name: analyze_regulatory_compliance\n          type: map\n          prompt: |\n            Analyze regulatory compliance issues in this legal document: {{ input.legal_document }}\n            Focus on identifying violations, required actions, and compliance deadlines.\n          output:\n            schema:\n              violations: "list[str]"\n              required_actions: "list[str]"\n              deadlines: "list[str]"\n\n        Example InstantiateSchema (what the agent should output):\n        {\n        "name": "extract_compliance_sections",\n        "code": \'\'\'\ndef transform(input_doc):\n    import re\n\n    legal_document = input_doc.get(\'legal_document\', \'\')\n\n    # Patterns to identify compliance-related content\n    compliance_patterns = [\n        r\'(?i)(violat[e|ion]|breach|non-complian[ce|t])\',\n        r\'(?i)(deadline|due date|expir[e|ation]|within.*days?)\',\n        r\'(?i)(shall|must|required|mandatory|obligation)\',\n        r\'(?i)(section|article|clause)\\s+\\d+.*(complian[ce|t]|regulat[e|ory])\'\n    ]\n\n    relevant_spans = []\n\n    # Find all matches and extract context around them\n    for pattern in compliance_patterns:\n        for match in re.finditer(pattern, legal_document):\n            start_pos = match.start()\n            end_pos = match.end()\n\n            # Extract 300 chars before and 800 chars after the match\n            context_start = max(0, start_pos - 300)\n            context_end = min(len(legal_document), end_pos + 800)\n\n            # Extract the context around the match\n            context = legal_document[context_start:context_end]\n            relevant_spans.append((context_start, context_end, context))\n\n    # Merge overlapping spans and remove duplicates\n    if relevant_spans:\n        # Sort by start position\n        relevant_spans.sort(key=lambda x: x[0])\n        merged_spans = [relevant_spans[0]]\n\n        for current_start, current_end, current_text in relevant_spans[1:]:\n            last_start, last_end, last_text = merged_spans[-1]\n\n            if current_start <= last_end + 100:  # Merge if close enough\n                # Extend the last span\n                new_end = max(last_end, current_end)\n                new_text = legal_document[last_start:new_end]\n                merged_spans[-1] = (last_start, new_end, new_text)\n            else:\n                merged_spans.append((current_start, current_end, current_text))\n\n        # Extract just the text portions\n        compressed_text = \'\\n\\n--- SECTION BREAK ---\\n\\n\'.join([span[2] for span in merged_spans])\n    else:\n        compressed_text = legal_document  # Fallback if no matches\n\n    return {\n        \'legal_document\': compressed_text\n    }\n            \'\'\'\n        }\n        ' test_cases=[DirectiveTestCase(name='detailed_merger_agreement_analysis', description='Should compress merger agreement for comprehensive legal analysis', input_config={'name': 'analyze_merger_agreement_terms', 'type': 'map', 'prompt': 'Perform a comprehensive legal analysis of this merger agreement: {{ input.merger_agreement }}\n\n                    Analyze and extract the following:\n                    1. Purchase price structure and payment mechanisms (cash, stock, earnouts, escrow arrangements)\n                    2. Material adverse change (MAC) definitions and carve-outs that could affect deal completion\n                    3. Representations and warranties with survival periods and liability caps\n                    4. Closing conditions precedent, including regulatory approvals and third-party consents\n                    5. Termination rights and associated breakup fees or reverse breakup fees\n                    6. Indemnification provisions including baskets, caps, and survival periods\n                    7. Employee retention arrangements and change-in-control provisions\n                    8. Integration planning requirements and operational restrictions during pendency\n                    9. Dispute resolution mechanisms and governing law provisions\n                    10. Post-closing adjustments and working capital mechanisms\n\n                    For each area, provide specific clause references, dollar amounts where applicable,\n                    time periods, and risk assessment (High/Medium/Low) with justification.', 'output': {'schema': {'purchase_price_analysis': 'string', 'mac_provisions': 'list[str]', 'representations_warranties': 'list[str]', 'closing_conditions': 'list[str]', 'termination_rights': 'string', 'indemnification_terms': 'string', 'employee_provisions': 'list[str]', 'integration_restrictions': 'list[str]', 'dispute_resolution': 'string', 'post_closing_adjustments': 'string', 'risk_assessment': 'string'}}}, target_ops=['analyze_merger_agreement_terms'], expected_behavior="Should add Code Map operation that extracts merger agreement sections using regex patterns for legal terms, financial provisions, and risk-related clauses. The return dictionary of the transform function should be {'merger_agreement': ....} only.", should_pass=True), DirectiveTestCase(name='multi_document_analysis_compression', description='Should compress document for multiple analysis operations', input_config=[{'name': 'extract_financial_metrics', 'type': 'map', 'prompt': 'Extract revenue, profit, and expense figures from: {{ input.earnings_report }}', 'output': {'schema': {'revenue': 'string', 'profit': 'string', 'expenses': 'list[str]'}}}, {'name': 'assess_financial_risks', 'type': 'map', 'prompt': 'Identify financial risks and warning signs in: {{ input.earnings_report }}', 'output': {'schema': {'risks': 'list[str]', 'warning_signs': 'list[str]'}}}], target_ops=['extract_financial_metrics', 'assess_financial_risks'], expected_behavior="Should add Code Map operation that extracts financial content needed for both operations. The return dictionary of the transform function should be {'earnings_report': ....} only.", should_pass=True)]
                Failure Type: simulation_execution_failure
                YAML Path: C:\Users\nickz\cuhk_app\docetl\results\moar_optimization\results\pipeline_openai_2-2-acc.yaml
                Error Message: Failed to execute plan C:\Users\nickz\cuhk_app\docetl\results\moar_optimization\results\pipeline_openai_2-2-acc.yaml: invalid group reference 1 at position 198
                ================================================================================
                
                ================================================================================
                Timestamp: 2026-01-26 10:21:55
                Node ID: 4-2-cost
                Parent ID: 4
                Latest Action: name='take_head_tail' formal_description='LLM_Op => Code Map -> LLM_Op' nl_description='Reduces document length by keeping only the first k words and optionally the last l words of the longest document field. This improves cost efficiency and can enhance accuracy for tasks that only require document beginnings (like classification).' when_to_use='When any LLM operation (Map, Filter, Reduce) only needs the beginning (and optionally end) of documents, such as classification tasks, filtering by document type, reducing document summaries, or when full document content causes accuracy issues due to too much context.' instantiate_schema_type=<class 'docetl.reasoning_optimizer.instantiate_schemas.TakeHeadTailInstantiateSchema'> example='\n        Original Map Operation (Research Paper Classification):\n        - name: classify_research_domain\n          type: map\n          prompt: |\n            What research domain does this paper belong to? Classify as Computer Science, Biology, Physics, Chemistry, or Other based on: {{ input.paper_text }}\n          output:\n            schema:\n              domain: "string"\n              confidence: "float"\n          model: gpt-4o-mini\n\n        TakeHeadTailInstantiateSchema:\n        {\n          "name": "extract_paper_abstract",\n          "document_key": "paper_text",\n          "head_words": 150,\n          "tail_words": 0\n        }\n\n        Resulting Pipeline:\n        - name: extract_paper_abstract\n          type: code_map\n          code: |\n            def transform(input_doc):\n                paper_text_content = input_doc.get(\'paper_text\', \'\')\n                words = paper_text_content.split()\n                if len(words) <= 150:\n                    truncated = paper_text_content\n                else:\n                    head = \' \'.join(words[:150])\n                    truncated = head\n                return {\'paper_text\': truncated}\n        - name: classify_research_domain\n          type: map\n          prompt: |\n            What research domain does this paper belong to? Classify as Computer Science, Biology, Physics, Chemistry, or Other based on: {{ input.paper_text }}\n          output:\n            schema:\n              domain: "string"\n              confidence: "float"\n          model: gpt-4o-mini\n        ' test_cases=[DirectiveTestCase(name='research_paper_classification', description='Classify research papers by domain using only abstract/introduction', input_config={'name': 'classify_paper_domain', 'type': 'map', 'prompt': 'What research domain does this paper belong to (CS, Biology, Physics, etc.)? Base your classification on the content: {{ input.full_text }}', 'output': {'schema': {'domain': 'string', 'confidence': 'float'}}, 'model': 'gpt-4o-mini'}, target_ops=['classify_paper_domain'], expected_behavior='Should truncate full_text to first ~150 words (abstract/intro) since paper classification only needs the beginning, not the full methodology/results sections', should_pass=True), DirectiveTestCase(name='document_metadata_extraction', description='Extract metadata from document headers/footers for indexing', input_config={'name': 'extract_document_metadata', 'type': 'map', 'prompt': 'Extract the title, author, date, and document type from this document: {{ input.content }}', 'output': {'schema': {'title': 'string', 'author': 'string', 'date': 'string', 'doc_type': 'string'}}, 'model': 'gpt-4o-mini'}, target_ops=['extract_document_metadata'], expected_behavior='Should keep both head (~100 words for headers/title) and tail (~50 words for footers/signatures) since metadata appears at document beginning and end', should_pass=True), DirectiveTestCase(name='email_priority_classification', description='Classify email priority using subject and first paragraph', input_config={'name': 'classify_email_priority', 'type': 'map', 'prompt': 'Classify this email as HIGH, MEDIUM, or LOW priority based on urgency indicators: {{ input.email_body }}', 'output': {'schema': {'priority': 'string', 'reasoning': 'string'}}, 'model': 'gpt-4o-mini'}, target_ops=['classify_email_priority'], expected_behavior='Should truncate email_body to first ~75 words since email priority is determined by subject line and opening, not full conversation thread', should_pass=True), DirectiveTestCase(name='legal_document_type_identification', description='Identify legal document type from contract headers and signature blocks', input_config={'name': 'identify_legal_doc_type', 'type': 'map', 'prompt': 'What type of legal document is this (contract, agreement, policy, etc.)? Analyze: {{ input.legal_text }}', 'output': {'schema': {'document_type': 'string', 'parties_involved': 'list[string]'}}, 'model': 'gpt-4o-mini'}, target_ops=['identify_legal_doc_type'], expected_behavior='Should keep head (~200 words for title/parties) and tail (~100 words for signature blocks) since legal doc type is indicated at beginning and parties sign at end', should_pass=True), DirectiveTestCase(name='spam_email_filtering', description='Filter out spam emails based on subject line and opening content', input_config={'name': 'filter_spam_emails', 'type': 'filter', 'prompt': 'Is this email spam? Look for suspicious patterns in: {{ input.email_content }}', 'output': {'schema': {'_bool': 'bool'}}, 'model': 'gpt-4o-mini'}, target_ops=['filter_spam_emails'], expected_behavior='Should truncate email_content to first ~100 words since spam detection relies on subject, sender, and opening content, not full email thread', should_pass=True), DirectiveTestCase(name='research_findings_synthesis', description='Reduce multiple research papers into a unified findings summary', input_config={'name': 'synthesize_research_findings', 'type': 'reduce', 'prompt': 'Synthesize the key findings from these research abstracts and conclusions: {% for doc in inputs %}{{ doc.paper_content }}{% endfor %}', 'output': {'schema': {'synthesis': 'string', 'key_themes': 'list[string]'}}, 'model': 'gpt-4o-mini'}, target_ops=['synthesize_research_findings'], expected_behavior='Should keep head (~200 words for abstracts) and tail (~150 words for conclusions) from each paper since synthesis needs both research goals and outcomes', should_pass=True)]
                Failure Type: execution_failure
                YAML Path: C:\Users\nickz\cuhk_app\docetl\results\moar_optimization\results\pipeline_openai_4-2-cost.yaml
                Error Message: Error parsing YAML configuration: while scanning a double-quoted scalar
  in "C:\Users\nickz\cuhk_app\docetl\results\moar_optimization\results\pipeline_openai_4-2-cost.yaml", line 19, column 11
found unknown escape character '\x00'
  in "C:\Users\nickz\cuhk_app\docetl\results\moar_optimization\results\pipeline_openai_4-2-cost.yaml", line 26, column 6
                ================================================================================
                
                ================================================================================
                Timestamp: 2026-01-26 10:21:55
                Node ID: 4-2-cost
                Parent ID: 4
                Latest Action: name='take_head_tail' formal_description='LLM_Op => Code Map -> LLM_Op' nl_description='Reduces document length by keeping only the first k words and optionally the last l words of the longest document field. This improves cost efficiency and can enhance accuracy for tasks that only require document beginnings (like classification).' when_to_use='When any LLM operation (Map, Filter, Reduce) only needs the beginning (and optionally end) of documents, such as classification tasks, filtering by document type, reducing document summaries, or when full document content causes accuracy issues due to too much context.' instantiate_schema_type=<class 'docetl.reasoning_optimizer.instantiate_schemas.TakeHeadTailInstantiateSchema'> example='\n        Original Map Operation (Research Paper Classification):\n        - name: classify_research_domain\n          type: map\n          prompt: |\n            What research domain does this paper belong to? Classify as Computer Science, Biology, Physics, Chemistry, or Other based on: {{ input.paper_text }}\n          output:\n            schema:\n              domain: "string"\n              confidence: "float"\n          model: gpt-4o-mini\n\n        TakeHeadTailInstantiateSchema:\n        {\n          "name": "extract_paper_abstract",\n          "document_key": "paper_text",\n          "head_words": 150,\n          "tail_words": 0\n        }\n\n        Resulting Pipeline:\n        - name: extract_paper_abstract\n          type: code_map\n          code: |\n            def transform(input_doc):\n                paper_text_content = input_doc.get(\'paper_text\', \'\')\n                words = paper_text_content.split()\n                if len(words) <= 150:\n                    truncated = paper_text_content\n                else:\n                    head = \' \'.join(words[:150])\n                    truncated = head\n                return {\'paper_text\': truncated}\n        - name: classify_research_domain\n          type: map\n          prompt: |\n            What research domain does this paper belong to? Classify as Computer Science, Biology, Physics, Chemistry, or Other based on: {{ input.paper_text }}\n          output:\n            schema:\n              domain: "string"\n              confidence: "float"\n          model: gpt-4o-mini\n        ' test_cases=[DirectiveTestCase(name='research_paper_classification', description='Classify research papers by domain using only abstract/introduction', input_config={'name': 'classify_paper_domain', 'type': 'map', 'prompt': 'What research domain does this paper belong to (CS, Biology, Physics, etc.)? Base your classification on the content: {{ input.full_text }}', 'output': {'schema': {'domain': 'string', 'confidence': 'float'}}, 'model': 'gpt-4o-mini'}, target_ops=['classify_paper_domain'], expected_behavior='Should truncate full_text to first ~150 words (abstract/intro) since paper classification only needs the beginning, not the full methodology/results sections', should_pass=True), DirectiveTestCase(name='document_metadata_extraction', description='Extract metadata from document headers/footers for indexing', input_config={'name': 'extract_document_metadata', 'type': 'map', 'prompt': 'Extract the title, author, date, and document type from this document: {{ input.content }}', 'output': {'schema': {'title': 'string', 'author': 'string', 'date': 'string', 'doc_type': 'string'}}, 'model': 'gpt-4o-mini'}, target_ops=['extract_document_metadata'], expected_behavior='Should keep both head (~100 words for headers/title) and tail (~50 words for footers/signatures) since metadata appears at document beginning and end', should_pass=True), DirectiveTestCase(name='email_priority_classification', description='Classify email priority using subject and first paragraph', input_config={'name': 'classify_email_priority', 'type': 'map', 'prompt': 'Classify this email as HIGH, MEDIUM, or LOW priority based on urgency indicators: {{ input.email_body }}', 'output': {'schema': {'priority': 'string', 'reasoning': 'string'}}, 'model': 'gpt-4o-mini'}, target_ops=['classify_email_priority'], expected_behavior='Should truncate email_body to first ~75 words since email priority is determined by subject line and opening, not full conversation thread', should_pass=True), DirectiveTestCase(name='legal_document_type_identification', description='Identify legal document type from contract headers and signature blocks', input_config={'name': 'identify_legal_doc_type', 'type': 'map', 'prompt': 'What type of legal document is this (contract, agreement, policy, etc.)? Analyze: {{ input.legal_text }}', 'output': {'schema': {'document_type': 'string', 'parties_involved': 'list[string]'}}, 'model': 'gpt-4o-mini'}, target_ops=['identify_legal_doc_type'], expected_behavior='Should keep head (~200 words for title/parties) and tail (~100 words for signature blocks) since legal doc type is indicated at beginning and parties sign at end', should_pass=True), DirectiveTestCase(name='spam_email_filtering', description='Filter out spam emails based on subject line and opening content', input_config={'name': 'filter_spam_emails', 'type': 'filter', 'prompt': 'Is this email spam? Look for suspicious patterns in: {{ input.email_content }}', 'output': {'schema': {'_bool': 'bool'}}, 'model': 'gpt-4o-mini'}, target_ops=['filter_spam_emails'], expected_behavior='Should truncate email_content to first ~100 words since spam detection relies on subject, sender, and opening content, not full email thread', should_pass=True), DirectiveTestCase(name='research_findings_synthesis', description='Reduce multiple research papers into a unified findings summary', input_config={'name': 'synthesize_research_findings', 'type': 'reduce', 'prompt': 'Synthesize the key findings from these research abstracts and conclusions: {% for doc in inputs %}{{ doc.paper_content }}{% endfor %}', 'output': {'schema': {'synthesis': 'string', 'key_themes': 'list[string]'}}, 'model': 'gpt-4o-mini'}, target_ops=['synthesize_research_findings'], expected_behavior='Should keep head (~200 words for abstracts) and tail (~150 words for conclusions) from each paper since synthesis needs both research goals and outcomes', should_pass=True)]
                Failure Type: simulation_execution_failure
                YAML Path: C:\Users\nickz\cuhk_app\docetl\results\moar_optimization\results\pipeline_openai_4-2-cost.yaml
                Error Message: Failed to execute plan C:\Users\nickz\cuhk_app\docetl\results\moar_optimization\results\pipeline_openai_4-2-cost.yaml: Error parsing YAML configuration: while scanning a double-quoted scalar
  in "C:\Users\nickz\cuhk_app\docetl\results\moar_optimization\results\pipeline_openai_4-2-cost.yaml", line 19, column 11
found unknown escape character '\x00'
  in "C:\Users\nickz\cuhk_app\docetl\results\moar_optimization\results\pipeline_openai_4-2-cost.yaml", line 26, column 6
                ================================================================================
                
                ================================================================================
                Timestamp: 2026-01-26 10:26:23
                Node ID: 9
                Parent ID: 4
                Latest Action: name='isolating_subtasks' formal_description='Map => Parallel Map -> Map' nl_description='Rewrites a single Map into a Parallel Map that isolates subtasks and generates separate outputs for each, followed by a Map that aggregates or synthesizes the results.' when_to_use='When the original Map is overloadedeither the prompt asks for many different things OR the output schema has many fieldsand subtasks are better handled independently (e.g., extract each attribute in parallel, then combine into a unified output).' instantiate_schema_type=<class 'docetl.reasoning_optimizer.instantiate_schemas.IsolatingSubtasksInstantiateSchema'> example='\n        Original Op (MapOpConfig):\n        - name: extract_contract_info\n          type: map\n          prompt: |\n            Extract the following from this contract: {{ input.document }}\n            - party names\n            - agreement date\n            - governing law\n            - termination clauses\n          output:\n            schema:\n              parties: "string"\n              agreement_date: "string"\n              governing_law: "string"\n              termination_clauses: "string"\n\n        Example InstantiateSchema (what the agent should output):\n        IsolatingSubtasksConfig(\n            subtasks=[\n                SubtaskConfig(\n                    name="Extract Basic Contract Info",\n                    prompt="Extract party names and agreement date from: {{ input.document }}",\n                    output_keys=["parties", "agreement_date"]\n                ),\n                SubtaskConfig(\n                    name="Extract Legal Terms",\n                    prompt="Extract governing law and termination clauses from: {{ input.document }}",\n                    output_keys=["governing_law", "termination_clauses"]\n                )\n            ],\n            aggregation_prompt="Combine the basic info {{ input.subtask_1_output }} with legal terms {{ input.subtask_2_output }} into the final contract summary."\n        )\n        ' test_cases=[DirectiveTestCase(name='complex_prompt_simple_output', description='Complex multi-task prompt but simple list output - should isolate by prompt complexity', input_config={'name': 'analyze_document', 'type': 'map', 'prompt': 'Analyze this document for: 1) sentiment and emotional tone, 2) key topics and themes, 3) factual accuracy and bias, 4) writing quality and readability, 5) actionable insights and recommendations. Document: {{ input.text }}', 'output': {'schema': {'results': 'list[string]'}}}, target_ops=['analyze_document'], expected_behavior='Should create >1 parallel map prompts covering all analysis aspects (sentiment, topics, bias, quality, insights) and aggregation prompt referencing all subtask outputs', should_pass=True), DirectiveTestCase(name='contract_analysis_many_fields', description='Legal contract extraction with many specific fields', input_config={'name': 'extract_contract_terms', 'type': 'map', 'prompt': 'Extract contract information from: {{ input.contract_text }}', 'output': {'schema': {'parties': 'string', 'agreement_date': 'string', 'governing_law': 'string', 'termination_clause': 'string', 'payment_terms': 'string', 'liability_cap': 'string'}}}, target_ops=['extract_contract_terms'], expected_behavior='Should create >1 parallel map prompts covering all 6 fields (parties, agreement_date, governing_law, termination_clause, payment_terms, liability_cap) and aggregation prompt referencing all subtask outputs', should_pass=True), DirectiveTestCase(name='medical_transcript_processing', description='Medical data extraction - different subtasks for different medical info types', input_config={'name': 'process_medical_record', 'type': 'map', 'prompt': 'Extract patient demographics, symptoms, diagnosis, and treatment plan from: {{ input.transcript }}', 'output': {'schema': {'patient_info': 'string', 'symptoms': 'string', 'diagnosis': 'string', 'treatment': 'string'}}}, target_ops=['process_medical_record'], expected_behavior='Should create >1 parallel map prompts covering all medical aspects (demographics, symptoms, diagnosis, treatment) and aggregation prompt referencing all subtask outputs', should_pass=True), DirectiveTestCase(name='research_paper_summary', description='Academic paper analysis with focus on different aspects', input_config={'name': 'summarize_research', 'type': 'map', 'prompt': 'Analyze this research paper for methodology, key findings, limitations, and practical applications: {{ input.paper_text }}', 'output': {'schema': {'summary': 'string', 'key_points': 'list[string]'}}}, target_ops=['summarize_research'], expected_behavior='Should create >1 parallel map prompts covering all research aspects (methodology, findings, limitations, applications) and aggregation prompt referencing all subtask outputs', should_pass=True), DirectiveTestCase(name='customer_feedback_analysis', description='Multi-aspect customer feedback analysis with simple output', input_config={'name': 'analyze_feedback', 'type': 'map', 'prompt': 'Analyze customer feedback for: product quality issues, service experience problems, pricing concerns, feature requests, and overall satisfaction. Feedback: {{ input.feedback_text }}', 'output': {'schema': {'insights': 'list[string]', 'priority_score': 'string'}}}, target_ops=['analyze_feedback'], expected_behavior='Should create >1 parallel map prompts covering all feedback aspects (quality, service, pricing, features, satisfaction) and aggregation prompt referencing all subtask outputs', should_pass=True), DirectiveTestCase(name='financial_report_extraction', description='Financial document with many specific metrics to extract', input_config={'name': 'extract_financial_data', 'type': 'map', 'prompt': 'Extract financial metrics from earnings report: {{ input.report }}', 'output': {'schema': {'revenue': 'string', 'profit_margin': 'string', 'cash_flow': 'string', 'debt_ratio': 'string', 'growth_rate': 'string', 'market_share': 'string'}}}, target_ops=['extract_financial_data'], expected_behavior='Should create >1 parallel map prompts covering all 6 financial metrics (revenue, profit_margin, cash_flow, debt_ratio, growth_rate, market_share) and aggregation prompt referencing all subtask outputs', should_pass=True)]
                Failure Type: execution_failure
                YAML Path: C:\Users\nickz\cuhk_app\docetl\results\moar_optimization\results\pipeline_openai_9.yaml
                Error Message: Syntax check failed for operation 'extract_fillers/extract_filler_aggregate': 1 validation error for schema
  Value error, If 'drop_keys' is not specified, both 'prompt' and 'output' must be present in the configuration [type=value_error, input_value={'name': 'extract_filler_...l': 'openai/gpt-5-nano'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.11/v/value_error
                ================================================================================
                
                ================================================================================
                Timestamp: 2026-01-26 10:26:23
                Node ID: 9
                Parent ID: 4
                Latest Action: name='isolating_subtasks' formal_description='Map => Parallel Map -> Map' nl_description='Rewrites a single Map into a Parallel Map that isolates subtasks and generates separate outputs for each, followed by a Map that aggregates or synthesizes the results.' when_to_use='When the original Map is overloadedeither the prompt asks for many different things OR the output schema has many fieldsand subtasks are better handled independently (e.g., extract each attribute in parallel, then combine into a unified output).' instantiate_schema_type=<class 'docetl.reasoning_optimizer.instantiate_schemas.IsolatingSubtasksInstantiateSchema'> example='\n        Original Op (MapOpConfig):\n        - name: extract_contract_info\n          type: map\n          prompt: |\n            Extract the following from this contract: {{ input.document }}\n            - party names\n            - agreement date\n            - governing law\n            - termination clauses\n          output:\n            schema:\n              parties: "string"\n              agreement_date: "string"\n              governing_law: "string"\n              termination_clauses: "string"\n\n        Example InstantiateSchema (what the agent should output):\n        IsolatingSubtasksConfig(\n            subtasks=[\n                SubtaskConfig(\n                    name="Extract Basic Contract Info",\n                    prompt="Extract party names and agreement date from: {{ input.document }}",\n                    output_keys=["parties", "agreement_date"]\n                ),\n                SubtaskConfig(\n                    name="Extract Legal Terms",\n                    prompt="Extract governing law and termination clauses from: {{ input.document }}",\n                    output_keys=["governing_law", "termination_clauses"]\n                )\n            ],\n            aggregation_prompt="Combine the basic info {{ input.subtask_1_output }} with legal terms {{ input.subtask_2_output }} into the final contract summary."\n        )\n        ' test_cases=[DirectiveTestCase(name='complex_prompt_simple_output', description='Complex multi-task prompt but simple list output - should isolate by prompt complexity', input_config={'name': 'analyze_document', 'type': 'map', 'prompt': 'Analyze this document for: 1) sentiment and emotional tone, 2) key topics and themes, 3) factual accuracy and bias, 4) writing quality and readability, 5) actionable insights and recommendations. Document: {{ input.text }}', 'output': {'schema': {'results': 'list[string]'}}}, target_ops=['analyze_document'], expected_behavior='Should create >1 parallel map prompts covering all analysis aspects (sentiment, topics, bias, quality, insights) and aggregation prompt referencing all subtask outputs', should_pass=True), DirectiveTestCase(name='contract_analysis_many_fields', description='Legal contract extraction with many specific fields', input_config={'name': 'extract_contract_terms', 'type': 'map', 'prompt': 'Extract contract information from: {{ input.contract_text }}', 'output': {'schema': {'parties': 'string', 'agreement_date': 'string', 'governing_law': 'string', 'termination_clause': 'string', 'payment_terms': 'string', 'liability_cap': 'string'}}}, target_ops=['extract_contract_terms'], expected_behavior='Should create >1 parallel map prompts covering all 6 fields (parties, agreement_date, governing_law, termination_clause, payment_terms, liability_cap) and aggregation prompt referencing all subtask outputs', should_pass=True), DirectiveTestCase(name='medical_transcript_processing', description='Medical data extraction - different subtasks for different medical info types', input_config={'name': 'process_medical_record', 'type': 'map', 'prompt': 'Extract patient demographics, symptoms, diagnosis, and treatment plan from: {{ input.transcript }}', 'output': {'schema': {'patient_info': 'string', 'symptoms': 'string', 'diagnosis': 'string', 'treatment': 'string'}}}, target_ops=['process_medical_record'], expected_behavior='Should create >1 parallel map prompts covering all medical aspects (demographics, symptoms, diagnosis, treatment) and aggregation prompt referencing all subtask outputs', should_pass=True), DirectiveTestCase(name='research_paper_summary', description='Academic paper analysis with focus on different aspects', input_config={'name': 'summarize_research', 'type': 'map', 'prompt': 'Analyze this research paper for methodology, key findings, limitations, and practical applications: {{ input.paper_text }}', 'output': {'schema': {'summary': 'string', 'key_points': 'list[string]'}}}, target_ops=['summarize_research'], expected_behavior='Should create >1 parallel map prompts covering all research aspects (methodology, findings, limitations, applications) and aggregation prompt referencing all subtask outputs', should_pass=True), DirectiveTestCase(name='customer_feedback_analysis', description='Multi-aspect customer feedback analysis with simple output', input_config={'name': 'analyze_feedback', 'type': 'map', 'prompt': 'Analyze customer feedback for: product quality issues, service experience problems, pricing concerns, feature requests, and overall satisfaction. Feedback: {{ input.feedback_text }}', 'output': {'schema': {'insights': 'list[string]', 'priority_score': 'string'}}}, target_ops=['analyze_feedback'], expected_behavior='Should create >1 parallel map prompts covering all feedback aspects (quality, service, pricing, features, satisfaction) and aggregation prompt referencing all subtask outputs', should_pass=True), DirectiveTestCase(name='financial_report_extraction', description='Financial document with many specific metrics to extract', input_config={'name': 'extract_financial_data', 'type': 'map', 'prompt': 'Extract financial metrics from earnings report: {{ input.report }}', 'output': {'schema': {'revenue': 'string', 'profit_margin': 'string', 'cash_flow': 'string', 'debt_ratio': 'string', 'growth_rate': 'string', 'market_share': 'string'}}}, target_ops=['extract_financial_data'], expected_behavior='Should create >1 parallel map prompts covering all 6 financial metrics (revenue, profit_margin, cash_flow, debt_ratio, growth_rate, market_share) and aggregation prompt referencing all subtask outputs', should_pass=True)]
                Failure Type: simulation_execution_failure
                YAML Path: C:\Users\nickz\cuhk_app\docetl\results\moar_optimization\results\pipeline_openai_9.yaml
                Error Message: Failed to execute plan C:\Users\nickz\cuhk_app\docetl\results\moar_optimization\results\pipeline_openai_9.yaml: Syntax check failed for operation 'extract_fillers/extract_filler_aggregate': 1 validation error for schema
  Value error, If 'drop_keys' is not specified, both 'prompt' and 'output' must be present in the configuration [type=value_error, input_value={'name': 'extract_filler_...l': 'openai/gpt-5-nano'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.11/v/value_error
                ================================================================================
                
                ================================================================================
                Timestamp: 2026-01-26 10:32:08
                Node ID: 2-2-acc
                Parent ID: 2
                Latest Action: name='doc_chunking_topk' formal_description='Map/Filter => Split -> TopK -> Reduce (-> Code Filter if original was Filter)' nl_description='Cost optimization directive for documents where only certain portions are relevant to the task (when at least half the document is irrelevant). Works with both Map and Filter operations. Transforms into a retrieval-augmented pipeline: splits documents into chunks, uses topk to retrieve the most relevant chunks, processes them in a reduce operation. For Filter operations, adds a final code_filter step to return boolean results. Ideal when processing full documents would be wasteful due to irrelevant content.' when_to_use="Use when only certain portions of documents are relevant to the task and at least half of the document content is irrelevant. Perfect for complex filters (e.g., 'does this review mention competitor products more favorably?') or targeted extraction from documents with localized relevant sections. Works with both Map (extraction) and Filter (boolean decision) operations. The retrieval step (embedding or FTS) finds the relevant chunks, avoiding processing irrelevant content. For filters, the final code_filter converts the reduce output to True/False." instantiate_schema_type=<class 'docetl.reasoning_optimizer.instantiate_schemas.DocumentChunkingTopKInstantiateSchema'> example='\n            # Example 1: Complex Filter on Long Customer Reviews\n            Original Op:\n            - name: filter_competitor_mentions\n              type: filter\n              prompt: |\n                Analyze this customer review to determine if it mentions competitor products\n                more positively than our product.\n\n                Our Product: {{ input.our_product }}\n                Review: {{ input.review_text }}\n                Review ID: {{ input.review_id }}\n\n                Return true if the review speaks more favorably about competitor products than ours.\n                Consider: feature comparisons, performance mentions, value assessments, recommendations.\n              output:\n                schema:\n                  mentions_competitors_more_positively: bool\n\n            InstantiateSchema (filter with embedding search):\n            {\n              "chunk_size": 10000,\n              "split_key": "review_text",\n              "reduce_prompt": "Analyze this customer review to determine if it mentions competitor products more positively than our product.\\n\\nOur Product: {{ inputs[0].our_product }}\\nReview: the top {{ inputs|length }} most relevant chunks from the document (ordered by relevance):\\n{% for input in inputs|sort(attribute=\'_topk_filter_competitor_mentions_chunks_rank\') %}\\nChunk (Rank {{ input._topk_filter_competitor_mentions_chunks_rank }}, Score {{ input._topk_filter_competitor_mentions_chunks_score }}):\\n{{ input.review_text_chunk }}\\n{% endfor %}\\nReview ID: {{ inputs[0].review_id }}\\n\\nReturn true if the review speaks more favorably about competitor products than ours.\\nConsider: feature comparisons, performance mentions, value assessments, recommendations.",\n              "topk_config": {\n                "method": "embedding",\n                "k": 5,\n                "query": "competitor comparison versus alternative better than superior inferior worse features performance value recommendation prefer instead",\n                "keys": ["review_text_chunk"],\n                "embedding_model": "text-embedding-3-small"\n              }\n            }\n\n            # Example 2: Map Operation - Extract Specific Sections from Long Documents\n            Original Op:\n            - name: extract_methodology_from_paper\n              type: map\n              prompt: |\n                Extract detailed methodology from this research paper:\n\n                Paper: {{ input.paper_content }}\n                Title: {{ input.title }}\n\n                Extract: study design, sample size, data collection methods,\n                statistical analyses, and validation approaches.\n              output:\n                schema:\n                  study_design: str\n                  sample_size: dict\n                  data_collection: list[str]\n                  statistical_methods: list[str]\n                  validation: str\n\n            InstantiateSchema (map with embedding search):\n            {\n              "chunk_size": 15000,\n              "split_key": "paper_content",\n              "reduce_prompt": "Extract detailed methodology from this research paper:\\n\\nPaper: the top {{ inputs|length }} most relevant chunks from the document (ordered by relevance):\\n{% for input in inputs|sort(attribute=\'_topk_extract_methodology_from_paper_chunks_rank\') %}\\nChunk (Rank {{ input._topk_extract_methodology_from_paper_chunks_rank }}, Score {{ input._topk_extract_methodology_from_paper_chunks_score }}):\\n{{ input.paper_content_chunk }}\\n{% endfor %}\\nTitle: {{ inputs[0].title }}\\n\\nExtract: study design, sample size, data collection methods, statistical analyses, and validation approaches.",\n              "topk_config": {\n                "method": "embedding",\n                "k": 8,\n                "query": "methodology methods study design sample size participants data collection statistical analysis validation procedure protocol experimental",\n                "keys": ["paper_content_chunk"],\n                "embedding_model": "text-embedding-3-small"\n              }\n            }\n\n            # Example 3: Filter with FTS - Check Contract Compliance\n            Original Op:\n            - name: filter_contracts_with_liability_caps\n              type: filter\n              prompt: |\n                Determine if this contract contains liability cap provisions\n                that limit damages to less than $1 million.\n\n                Contract: {{ input.contract_text }}\n                Contract ID: {{ input.contract_id }}\n                Party: {{ input.counterparty }}\n\n                Return true if contract caps liability below $1M, false otherwise.\n              output:\n                schema:\n                  has_low_liability_cap: bool\n\n            InstantiateSchema (filter with FTS for legal terms):\n            {\n              "chunk_size": 12000,\n              "split_key": "contract_text",\n              "reduce_prompt": "Determine if this contract contains liability cap provisions that limit damages to less than $1 million.\\n\\nContract: the top {{ inputs|length }} most relevant chunks from the document (ordered by relevance):\\n{% for input in inputs|sort(attribute=\'_topk_filter_contracts_with_liability_caps_chunks_rank\') %}\\nSection (Rank {{ input._topk_filter_contracts_with_liability_caps_chunks_rank }}, Score {{ input._topk_filter_contracts_with_liability_caps_chunks_score }}):\\n{{ input.contract_text_chunk }}\\n{% endfor %}\\nContract ID: {{ inputs[0].contract_id }}\\nParty: {{ inputs[0].counterparty }}\\n\\nReturn true if contract caps liability below $1M, false otherwise.",\n              "topk_config": {\n                "method": "fts",\n                "k": 10,\n                "query": "liability limitation cap maximum damages indirect consequential million dollars aggregate total exposure indemnification",\n                "keys": ["contract_text_chunk"]\n              }\n            }\n        ' test_cases=[DirectiveTestCase(name='clinical_trial_adverse_events_extraction', description='Should transform clinical trial safety analysis into chunking pipeline with embedding-based topk for thematic content', input_config={'name': 'extract_clinical_trial_safety', 'type': 'map', 'prompt': 'Analyze this clinical trial protocol and safety report to extract comprehensive safety information:\n\n                    Protocol Number: {{ input.protocol_id }}\n                    Study Phase: {{ input.study_phase }}\n                    Document: {{ input.trial_document }}\n\n                    Extract and analyze:\n                    1. ALL adverse events (AEs) with:\n                       - Event description and medical terminology (MedDRA preferred terms)\n                       - Severity grade (1-5 per CTCAE v5.0)\n                       - Relationship to study drug (definitely, probably, possibly, unlikely, not related)\n                       - Onset timing relative to treatment start\n                       - Resolution status and duration\n                       - Actions taken (dose reduced, interrupted, discontinued)\n\n                    2. Serious adverse events (SAEs) with additional details:\n                       - Hospitalization requirements\n                       - Life-threatening classification\n                       - Death outcomes with causality assessment\n                       - Expedited reporting timeline compliance\n\n                    3. Laboratory abnormalities:\n                       - Clinically significant lab value shifts\n                       - Grade 3/4 laboratory toxicities\n                       - Liver function test elevations (ALT, AST, bilirubin)\n                       - Renal function changes (creatinine, eGFR)\n                       - Hematologic abnormalities\n\n                    4. Dose-limiting toxicities (DLTs) and maximum tolerated dose (MTD) determination\n\n                    5. Safety run-in period results if applicable\n\n                    6. Data safety monitoring board (DSMB) recommendations and protocol modifications\n\n                    Ensure all safety data is captured with appropriate medical coding and regulatory compliance.', 'output': {'schema': {'adverse_events': 'list[dict]', 'serious_adverse_events': 'list[dict]', 'lab_abnormalities': 'list[dict]', 'dose_limiting_toxicities': 'list[dict]', 'dsmb_recommendations': 'list[str]', 'safety_summary': 'dict'}}}, target_ops=['extract_clinical_trial_safety'], expected_behavior='Should create chunking pipeline with topk using embedding search to find sections discussing adverse events, safety data, laboratory results, and DSMB recommendations. Chunks should be 5-8k tokens with k=10-15 to capture all safety-related sections', should_pass=True), DirectiveTestCase(name='sec_filing_risk_factors_extraction', description='Should transform SEC filing analysis into chunking pipeline with FTS-based topk for specific regulatory terms', input_config={'name': 'extract_sec_risk_disclosures', 'type': 'map', 'prompt': 'Extract and analyze all risk factor disclosures from this SEC 10-K filing:\n\n                    Company: {{ input.company_ticker }}\n                    Filing Period: {{ input.filing_period }}\n                    Document: {{ input.form_10k }}\n                    Industry: {{ input.industry_classification }}\n\n                    Identify and categorize:\n\n                    1. BUSINESS AND OPERATIONAL RISKS:\n                       - Supply chain vulnerabilities and dependencies\n                       - Key customer concentration (customers >10% revenue)\n                       - Competition and market share threats\n                       - Product obsolescence and innovation risks\n                       - Manufacturing and quality control risks\n                       - Intellectual property disputes and patent expirations\n\n                    2. FINANCIAL AND MARKET RISKS:\n                       - Liquidity and cash flow concerns\n                       - Debt covenants and refinancing risks\n                       - Foreign exchange exposure by currency\n                       - Interest rate sensitivity analysis\n                       - Credit risk and counterparty exposure\n                       - Goodwill and intangible asset impairment risks\n\n                    3. REGULATORY AND COMPLIANCE RISKS:\n                       - SEC investigation disclosures\n                       - FDA/regulatory approval dependencies\n                       - Environmental liabilities and remediation costs\n                       - Tax disputes and uncertain tax positions\n                       - FCPA and anti-corruption compliance\n                       - Data privacy (GDPR, CCPA) obligations\n\n                    4. CYBERSECURITY AND TECHNOLOGY RISKS:\n                       - Data breach history and potential impacts\n                       - IT system dependencies and modernization needs\n                       - Third-party technology provider risks\n                       - Business continuity and disaster recovery\n\n                    5. LITIGATION AND LEGAL RISKS:\n                       - Material pending litigation with potential damages\n                       - Class action lawsuit exposure\n                       - Warranty and product liability claims\n                       - Employment and labor disputes\n\n                    6. ESG AND REPUTATIONAL RISKS:\n                       - Climate change physical and transition risks\n                       - Social license to operate concerns\n                       - Executive succession planning\n                       - Related party transaction risks\n\n                    For each risk, extract:\n                    - Risk description and specific company exposure\n                    - Quantitative impact estimates if disclosed\n                    - Mitigation strategies mentioned\n                    - Changes from prior year disclosure\n                    - Forward-looking statements and warnings', 'output': {'schema': {'business_operational_risks': 'list[dict]', 'financial_market_risks': 'list[dict]', 'regulatory_compliance_risks': 'list[dict]', 'cybersecurity_technology_risks': 'list[dict]', 'litigation_legal_risks': 'list[dict]', 'esg_reputational_risks': 'list[dict]', 'risk_factor_changes': 'list[dict]', 'material_risk_summary': 'dict'}}}, target_ops=['extract_sec_risk_disclosures'], expected_behavior='Should create chunking pipeline with topk using FTS to search for specific risk-related keywords and sections (Item 1A, risk factors, legal proceedings, etc.). Chunks should be 6-10k tokens with k=15-20 to ensure comprehensive risk coverage', should_pass=True), DirectiveTestCase(name='insurance_claim_analysis_with_dynamic_query', description='Should transform insurance claim analysis with Jinja template query based on claim type', input_config={'name': 'analyze_insurance_claim', 'type': 'map', 'prompt': 'Analyze this insurance claim file for coverage determination and fraud indicators:\n\n                    Claim Number: {{ input.claim_id }}\n                    Policy Number: {{ input.policy_number }}\n                    Claim Type: {{ input.claim_type }}\n                    Claimed Amount: {{ input.claimed_amount }}\n                    Policy Documents: {{ input.policy_documents }}\n                    Claim Documents: {{ input.claim_submission }}\n                    Prior Claims History: {{ input.claims_history }}\n\n                    Perform comprehensive analysis:\n\n                    1. COVERAGE DETERMINATION:\n                       - Verify incident date falls within policy period\n                       - Check specific peril coverage for {{ input.claim_type }} claims\n                       - Identify applicable policy limits and sublimits\n                       - Calculate deductibles and co-insurance\n                       - Review exclusions that may apply\n                       - Assess pre-existing condition clauses (if medical)\n                       - Verify additional living expense limits (if property)\n\n                    2. CLAIM VALIDATION:\n                       - Cross-reference damage description with photos/evidence\n                       - Verify repair estimates against market rates\n                       - Validate medical treatment necessity and coding\n                       - Check for duplicate submissions or double-dipping\n                       - Verify loss circumstances match policy terms\n\n                    3. FRAUD INDICATORS ASSESSMENT:\n                       - Pattern analysis against known fraud schemes\n                       - Inconsistencies in statements or documentation\n                       - Suspicious timing (policy inception, premium issues)\n                       - Inflated valuations or treatment costs\n                       - Missing or altered documentation\n                       - Prior suspicious claims pattern\n\n                    4. THIRD-PARTY LIABILITY:\n                       - Subrogation opportunities\n                       - Other insurance coverage available\n                       - Responsible party identification\n                       - Coordination of benefits requirements\n\n                    5. REGULATORY COMPLIANCE:\n                       - State-specific claim handling requirements\n                       - Unfair claim settlement practices act compliance\n                       - Required notices and timelines\n                       - Bad faith claim indicators\n\n                    6. SETTLEMENT RECOMMENDATION:\n                       - Covered amount calculation\n                       - Recommended settlement range\n                       - Payment breakdown by category\n                       - Reserve recommendations\n                       - Special investigation unit (SIU) referral if warranted', 'output': {'schema': {'coverage_analysis': 'dict', 'claim_validation': 'dict', 'fraud_indicators': 'list[dict]', 'third_party_liability': 'dict', 'regulatory_compliance': 'dict', 'settlement_recommendation': 'dict', 'siu_referral': 'bool', 'reserve_amount': 'float'}}}, target_ops=['analyze_insurance_claim'], expected_behavior="Should create chunking pipeline with topk using dynamic Jinja query that incorporates claim_type to search for relevant policy sections and prior claims. Query should adapt based on whether it's property, auto, medical, or liability claim. Chunks should be 5-7k tokens with k=12-18", should_pass=True)]
                Failure Type: execution_failure
                YAML Path: C:\Users\nickz\cuhk_app\docetl\results\moar_optimization\results\pipeline_openai_2-2-acc.yaml
                Error Message: 'dict object' has no attribute '_topk_extract_filler_emb_rank'
Your prompt can include the following variables: ['reduce_key', 'inputs', 'retrieval_context']
For dictionary variables, you can access keys using dot notation (e.g. input.key).
Available keys for each document: {'reduce_key': ['split_extract_filler_id'], 'inputs': ['inputs[i].year', 'inputs[i].date', 'inputs[i].title', 'inputs[i].content', 'inputs[i].id', 'inputs[i].content_chunk', 'inputs[i].split_extract_filler_id', 'inputs[i].split_extract_filler_chunk_num', 'inputs[i]._topk_extract_filler_chunks_rank', 'inputs[i]._topk_extract_filler_chunks_score']}

                ================================================================================
                
                ================================================================================
                Timestamp: 2026-01-26 10:32:08
                Node ID: 2-2-acc
                Parent ID: 2
                Latest Action: name='doc_chunking_topk' formal_description='Map/Filter => Split -> TopK -> Reduce (-> Code Filter if original was Filter)' nl_description='Cost optimization directive for documents where only certain portions are relevant to the task (when at least half the document is irrelevant). Works with both Map and Filter operations. Transforms into a retrieval-augmented pipeline: splits documents into chunks, uses topk to retrieve the most relevant chunks, processes them in a reduce operation. For Filter operations, adds a final code_filter step to return boolean results. Ideal when processing full documents would be wasteful due to irrelevant content.' when_to_use="Use when only certain portions of documents are relevant to the task and at least half of the document content is irrelevant. Perfect for complex filters (e.g., 'does this review mention competitor products more favorably?') or targeted extraction from documents with localized relevant sections. Works with both Map (extraction) and Filter (boolean decision) operations. The retrieval step (embedding or FTS) finds the relevant chunks, avoiding processing irrelevant content. For filters, the final code_filter converts the reduce output to True/False." instantiate_schema_type=<class 'docetl.reasoning_optimizer.instantiate_schemas.DocumentChunkingTopKInstantiateSchema'> example='\n            # Example 1: Complex Filter on Long Customer Reviews\n            Original Op:\n            - name: filter_competitor_mentions\n              type: filter\n              prompt: |\n                Analyze this customer review to determine if it mentions competitor products\n                more positively than our product.\n\n                Our Product: {{ input.our_product }}\n                Review: {{ input.review_text }}\n                Review ID: {{ input.review_id }}\n\n                Return true if the review speaks more favorably about competitor products than ours.\n                Consider: feature comparisons, performance mentions, value assessments, recommendations.\n              output:\n                schema:\n                  mentions_competitors_more_positively: bool\n\n            InstantiateSchema (filter with embedding search):\n            {\n              "chunk_size": 10000,\n              "split_key": "review_text",\n              "reduce_prompt": "Analyze this customer review to determine if it mentions competitor products more positively than our product.\\n\\nOur Product: {{ inputs[0].our_product }}\\nReview: the top {{ inputs|length }} most relevant chunks from the document (ordered by relevance):\\n{% for input in inputs|sort(attribute=\'_topk_filter_competitor_mentions_chunks_rank\') %}\\nChunk (Rank {{ input._topk_filter_competitor_mentions_chunks_rank }}, Score {{ input._topk_filter_competitor_mentions_chunks_score }}):\\n{{ input.review_text_chunk }}\\n{% endfor %}\\nReview ID: {{ inputs[0].review_id }}\\n\\nReturn true if the review speaks more favorably about competitor products than ours.\\nConsider: feature comparisons, performance mentions, value assessments, recommendations.",\n              "topk_config": {\n                "method": "embedding",\n                "k": 5,\n                "query": "competitor comparison versus alternative better than superior inferior worse features performance value recommendation prefer instead",\n                "keys": ["review_text_chunk"],\n                "embedding_model": "text-embedding-3-small"\n              }\n            }\n\n            # Example 2: Map Operation - Extract Specific Sections from Long Documents\n            Original Op:\n            - name: extract_methodology_from_paper\n              type: map\n              prompt: |\n                Extract detailed methodology from this research paper:\n\n                Paper: {{ input.paper_content }}\n                Title: {{ input.title }}\n\n                Extract: study design, sample size, data collection methods,\n                statistical analyses, and validation approaches.\n              output:\n                schema:\n                  study_design: str\n                  sample_size: dict\n                  data_collection: list[str]\n                  statistical_methods: list[str]\n                  validation: str\n\n            InstantiateSchema (map with embedding search):\n            {\n              "chunk_size": 15000,\n              "split_key": "paper_content",\n              "reduce_prompt": "Extract detailed methodology from this research paper:\\n\\nPaper: the top {{ inputs|length }} most relevant chunks from the document (ordered by relevance):\\n{% for input in inputs|sort(attribute=\'_topk_extract_methodology_from_paper_chunks_rank\') %}\\nChunk (Rank {{ input._topk_extract_methodology_from_paper_chunks_rank }}, Score {{ input._topk_extract_methodology_from_paper_chunks_score }}):\\n{{ input.paper_content_chunk }}\\n{% endfor %}\\nTitle: {{ inputs[0].title }}\\n\\nExtract: study design, sample size, data collection methods, statistical analyses, and validation approaches.",\n              "topk_config": {\n                "method": "embedding",\n                "k": 8,\n                "query": "methodology methods study design sample size participants data collection statistical analysis validation procedure protocol experimental",\n                "keys": ["paper_content_chunk"],\n                "embedding_model": "text-embedding-3-small"\n              }\n            }\n\n            # Example 3: Filter with FTS - Check Contract Compliance\n            Original Op:\n            - name: filter_contracts_with_liability_caps\n              type: filter\n              prompt: |\n                Determine if this contract contains liability cap provisions\n                that limit damages to less than $1 million.\n\n                Contract: {{ input.contract_text }}\n                Contract ID: {{ input.contract_id }}\n                Party: {{ input.counterparty }}\n\n                Return true if contract caps liability below $1M, false otherwise.\n              output:\n                schema:\n                  has_low_liability_cap: bool\n\n            InstantiateSchema (filter with FTS for legal terms):\n            {\n              "chunk_size": 12000,\n              "split_key": "contract_text",\n              "reduce_prompt": "Determine if this contract contains liability cap provisions that limit damages to less than $1 million.\\n\\nContract: the top {{ inputs|length }} most relevant chunks from the document (ordered by relevance):\\n{% for input in inputs|sort(attribute=\'_topk_filter_contracts_with_liability_caps_chunks_rank\') %}\\nSection (Rank {{ input._topk_filter_contracts_with_liability_caps_chunks_rank }}, Score {{ input._topk_filter_contracts_with_liability_caps_chunks_score }}):\\n{{ input.contract_text_chunk }}\\n{% endfor %}\\nContract ID: {{ inputs[0].contract_id }}\\nParty: {{ inputs[0].counterparty }}\\n\\nReturn true if contract caps liability below $1M, false otherwise.",\n              "topk_config": {\n                "method": "fts",\n                "k": 10,\n                "query": "liability limitation cap maximum damages indirect consequential million dollars aggregate total exposure indemnification",\n                "keys": ["contract_text_chunk"]\n              }\n            }\n        ' test_cases=[DirectiveTestCase(name='clinical_trial_adverse_events_extraction', description='Should transform clinical trial safety analysis into chunking pipeline with embedding-based topk for thematic content', input_config={'name': 'extract_clinical_trial_safety', 'type': 'map', 'prompt': 'Analyze this clinical trial protocol and safety report to extract comprehensive safety information:\n\n                    Protocol Number: {{ input.protocol_id }}\n                    Study Phase: {{ input.study_phase }}\n                    Document: {{ input.trial_document }}\n\n                    Extract and analyze:\n                    1. ALL adverse events (AEs) with:\n                       - Event description and medical terminology (MedDRA preferred terms)\n                       - Severity grade (1-5 per CTCAE v5.0)\n                       - Relationship to study drug (definitely, probably, possibly, unlikely, not related)\n                       - Onset timing relative to treatment start\n                       - Resolution status and duration\n                       - Actions taken (dose reduced, interrupted, discontinued)\n\n                    2. Serious adverse events (SAEs) with additional details:\n                       - Hospitalization requirements\n                       - Life-threatening classification\n                       - Death outcomes with causality assessment\n                       - Expedited reporting timeline compliance\n\n                    3. Laboratory abnormalities:\n                       - Clinically significant lab value shifts\n                       - Grade 3/4 laboratory toxicities\n                       - Liver function test elevations (ALT, AST, bilirubin)\n                       - Renal function changes (creatinine, eGFR)\n                       - Hematologic abnormalities\n\n                    4. Dose-limiting toxicities (DLTs) and maximum tolerated dose (MTD) determination\n\n                    5. Safety run-in period results if applicable\n\n                    6. Data safety monitoring board (DSMB) recommendations and protocol modifications\n\n                    Ensure all safety data is captured with appropriate medical coding and regulatory compliance.', 'output': {'schema': {'adverse_events': 'list[dict]', 'serious_adverse_events': 'list[dict]', 'lab_abnormalities': 'list[dict]', 'dose_limiting_toxicities': 'list[dict]', 'dsmb_recommendations': 'list[str]', 'safety_summary': 'dict'}}}, target_ops=['extract_clinical_trial_safety'], expected_behavior='Should create chunking pipeline with topk using embedding search to find sections discussing adverse events, safety data, laboratory results, and DSMB recommendations. Chunks should be 5-8k tokens with k=10-15 to capture all safety-related sections', should_pass=True), DirectiveTestCase(name='sec_filing_risk_factors_extraction', description='Should transform SEC filing analysis into chunking pipeline with FTS-based topk for specific regulatory terms', input_config={'name': 'extract_sec_risk_disclosures', 'type': 'map', 'prompt': 'Extract and analyze all risk factor disclosures from this SEC 10-K filing:\n\n                    Company: {{ input.company_ticker }}\n                    Filing Period: {{ input.filing_period }}\n                    Document: {{ input.form_10k }}\n                    Industry: {{ input.industry_classification }}\n\n                    Identify and categorize:\n\n                    1. BUSINESS AND OPERATIONAL RISKS:\n                       - Supply chain vulnerabilities and dependencies\n                       - Key customer concentration (customers >10% revenue)\n                       - Competition and market share threats\n                       - Product obsolescence and innovation risks\n                       - Manufacturing and quality control risks\n                       - Intellectual property disputes and patent expirations\n\n                    2. FINANCIAL AND MARKET RISKS:\n                       - Liquidity and cash flow concerns\n                       - Debt covenants and refinancing risks\n                       - Foreign exchange exposure by currency\n                       - Interest rate sensitivity analysis\n                       - Credit risk and counterparty exposure\n                       - Goodwill and intangible asset impairment risks\n\n                    3. REGULATORY AND COMPLIANCE RISKS:\n                       - SEC investigation disclosures\n                       - FDA/regulatory approval dependencies\n                       - Environmental liabilities and remediation costs\n                       - Tax disputes and uncertain tax positions\n                       - FCPA and anti-corruption compliance\n                       - Data privacy (GDPR, CCPA) obligations\n\n                    4. CYBERSECURITY AND TECHNOLOGY RISKS:\n                       - Data breach history and potential impacts\n                       - IT system dependencies and modernization needs\n                       - Third-party technology provider risks\n                       - Business continuity and disaster recovery\n\n                    5. LITIGATION AND LEGAL RISKS:\n                       - Material pending litigation with potential damages\n                       - Class action lawsuit exposure\n                       - Warranty and product liability claims\n                       - Employment and labor disputes\n\n                    6. ESG AND REPUTATIONAL RISKS:\n                       - Climate change physical and transition risks\n                       - Social license to operate concerns\n                       - Executive succession planning\n                       - Related party transaction risks\n\n                    For each risk, extract:\n                    - Risk description and specific company exposure\n                    - Quantitative impact estimates if disclosed\n                    - Mitigation strategies mentioned\n                    - Changes from prior year disclosure\n                    - Forward-looking statements and warnings', 'output': {'schema': {'business_operational_risks': 'list[dict]', 'financial_market_risks': 'list[dict]', 'regulatory_compliance_risks': 'list[dict]', 'cybersecurity_technology_risks': 'list[dict]', 'litigation_legal_risks': 'list[dict]', 'esg_reputational_risks': 'list[dict]', 'risk_factor_changes': 'list[dict]', 'material_risk_summary': 'dict'}}}, target_ops=['extract_sec_risk_disclosures'], expected_behavior='Should create chunking pipeline with topk using FTS to search for specific risk-related keywords and sections (Item 1A, risk factors, legal proceedings, etc.). Chunks should be 6-10k tokens with k=15-20 to ensure comprehensive risk coverage', should_pass=True), DirectiveTestCase(name='insurance_claim_analysis_with_dynamic_query', description='Should transform insurance claim analysis with Jinja template query based on claim type', input_config={'name': 'analyze_insurance_claim', 'type': 'map', 'prompt': 'Analyze this insurance claim file for coverage determination and fraud indicators:\n\n                    Claim Number: {{ input.claim_id }}\n                    Policy Number: {{ input.policy_number }}\n                    Claim Type: {{ input.claim_type }}\n                    Claimed Amount: {{ input.claimed_amount }}\n                    Policy Documents: {{ input.policy_documents }}\n                    Claim Documents: {{ input.claim_submission }}\n                    Prior Claims History: {{ input.claims_history }}\n\n                    Perform comprehensive analysis:\n\n                    1. COVERAGE DETERMINATION:\n                       - Verify incident date falls within policy period\n                       - Check specific peril coverage for {{ input.claim_type }} claims\n                       - Identify applicable policy limits and sublimits\n                       - Calculate deductibles and co-insurance\n                       - Review exclusions that may apply\n                       - Assess pre-existing condition clauses (if medical)\n                       - Verify additional living expense limits (if property)\n\n                    2. CLAIM VALIDATION:\n                       - Cross-reference damage description with photos/evidence\n                       - Verify repair estimates against market rates\n                       - Validate medical treatment necessity and coding\n                       - Check for duplicate submissions or double-dipping\n                       - Verify loss circumstances match policy terms\n\n                    3. FRAUD INDICATORS ASSESSMENT:\n                       - Pattern analysis against known fraud schemes\n                       - Inconsistencies in statements or documentation\n                       - Suspicious timing (policy inception, premium issues)\n                       - Inflated valuations or treatment costs\n                       - Missing or altered documentation\n                       - Prior suspicious claims pattern\n\n                    4. THIRD-PARTY LIABILITY:\n                       - Subrogation opportunities\n                       - Other insurance coverage available\n                       - Responsible party identification\n                       - Coordination of benefits requirements\n\n                    5. REGULATORY COMPLIANCE:\n                       - State-specific claim handling requirements\n                       - Unfair claim settlement practices act compliance\n                       - Required notices and timelines\n                       - Bad faith claim indicators\n\n                    6. SETTLEMENT RECOMMENDATION:\n                       - Covered amount calculation\n                       - Recommended settlement range\n                       - Payment breakdown by category\n                       - Reserve recommendations\n                       - Special investigation unit (SIU) referral if warranted', 'output': {'schema': {'coverage_analysis': 'dict', 'claim_validation': 'dict', 'fraud_indicators': 'list[dict]', 'third_party_liability': 'dict', 'regulatory_compliance': 'dict', 'settlement_recommendation': 'dict', 'siu_referral': 'bool', 'reserve_amount': 'float'}}}, target_ops=['analyze_insurance_claim'], expected_behavior="Should create chunking pipeline with topk using dynamic Jinja query that incorporates claim_type to search for relevant policy sections and prior claims. Query should adapt based on whether it's property, auto, medical, or liability claim. Chunks should be 5-7k tokens with k=12-18", should_pass=True)]
                Failure Type: simulation_execution_failure
                YAML Path: C:\Users\nickz\cuhk_app\docetl\results\moar_optimization\results\pipeline_openai_2-2-acc.yaml
                Error Message: Failed to execute plan C:\Users\nickz\cuhk_app\docetl\results\moar_optimization\results\pipeline_openai_2-2-acc.yaml: 'dict object' has no attribute '_topk_extract_filler_emb_rank'
Your prompt can include the following variables: ['reduce_key', 'inputs', 'retrieval_context']
For dictionary variables, you can access keys using dot notation (e.g. input.key).
Available keys for each document: {'reduce_key': ['split_extract_filler_id'], 'inputs': ['inputs[i].year', 'inputs[i].date', 'inputs[i].title', 'inputs[i].content', 'inputs[i].id', 'inputs[i].content_chunk', 'inputs[i].split_extract_filler_id', 'inputs[i].split_extract_filler_chunk_num', 'inputs[i]._topk_extract_filler_chunks_rank', 'inputs[i]._topk_extract_filler_chunks_score']}

                ================================================================================
                