datasets:
  transcripts:
    path: C:\Users\nickz\cuhk_app\docetl\data_sample.json
    type: file
default_model: openai/gpt-5-nano
bypass_cache: true
operations:
- name: split_extract_filler
  type: split
  split_key: content
  method: token_count
  method_kwargs:
    num_tokens: 12000
    model: openai/gpt-5-nano
- name: topk_extract_filler_chunks
  type: topk
  method: fts
  k: 8
  keys:
  - content_chunk
  query: um uh ah hmm you know I mean like well so and so you see let me frankly by
    the way 'you know' 'I mean' 'let me' 'by the way'
  stratify_key:
  - split_extract_filler_id
- name: reduce_extract_filler
  type: reduce
  reduce_key: split_extract_filler_id
  prompt: 'Extract all instances of filler words spoken in: the top {{ inputs|length
    }} most relevant chunks from the document (ordered by relevance):

    {% for input in inputs|sort(attribute=''_topk_extract_filler_chunks_rank'') %}

    Chunk (Rank {{ input._topk_extract_filler_chunks_rank }}, Score {{ input._topk_extract_filler_chunks_score
    }}):

    {{ input.content_chunk }}

    {% endfor %}

    '
  model: openai/gpt-5-nano
  litellm_completion_kwargs:
    temperature: 0
  output:
    schema:
      filler_words: list[str]
  associative: false
  pass_through: true
pipeline:
  steps:
  - name: extract_fillers
    input: transcripts
    operations:
    - split_extract_filler
    - topk_extract_filler_chunks
    - reduce_extract_filler
  output:
    type: file
    path: C:\Users\nickz\cuhk_app\docetl\results\moar_optimization\results1\pipeline_reproduce1_openai_11.json
optimizer_config:
  type: moar
  save_dir: results/moar_optimization/results1
  available_models:
  - openai/gpt-5-mini
  - openai/gpt-5-nano
  evaluation_file: evaluate_fillers.py
  metric_key: filler_extraction_score
  max_iterations: 10
  rewrite_agent_model: openai/gpt-5-mini
  model: openai/gpt-5-mini
rate_limits:
  llm_call:
  - count: 500
    per: 1
    unit: minute
  llm_tokens:
  - count: 500000
    per: 1
    unit: minute
